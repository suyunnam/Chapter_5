---
title: "Chapter 5. Estimating quantum yield of PSII using Environ data"
author: "Suyun Nam"
date: "04.30.2025"
format: html
---

# Objectives
Develop a machine learning analysis to create models that predict quantum yield of PSII for red lettuce as a function of meteorological variables

Meteorological variables to use
1) ePPFD (4 rep; 15 min averaged & Instantaneous)
2) PPFD (4 rep; 15 min averaged & Instantaneous)
3) Temperature
4) Vapor pressure deficit
5) CO2 level

Secondary variables
- DLI until now
- eDLI until now


# Fix directoroy to "Chapter_5"
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = normalizePath("../.."))
```

# Libraries
```{r}
# Loading packages
library(readxl)      # reading excel file
library(tidyverse)   # data wrangling and plotting
library(tidymodels)  # ML workflows
library(lubridate)   # working with dates and times
library(ggcorrplot)  # correlation matrix plots
library(broom)       # model coefficient organizing
library(vip)         # variable importance plotting
library(car)         # multicollinearity check
library(glmnet)      # elastic net
library(tune)        # Auto tune for hyperparameters
```

# Import data
## Import Environ data
```{r}
# Import the original excel file
environ_raw <- read_excel("data/0422/environ_excel.xlsx")
environ_raw

# Save each sheet as csv file
write.csv(environ_raw, file = "data/0422/environ.csv", row.names = FALSE)
environ <- read_csv("data/0422/environ.csv")
environ
```

## Import CF data
```{r}
# Import the original excel file
cf_excel <- read_excel("data/0422/cf_excel.xlsx")
cf_excel

# Save each sheet as csv file
write.csv(cf_excel, file = "data/0422/cf.csv", row.names = FALSE)
cf <- read_csv("data/0422/cf.csv")
cf
```


# Wrangling data
## CF
Round time to 15-min interval and remove night time data (cf)
```{r}
cf_dfw <- cf %>%
  mutate(time = round_date(time, unit = "15 minutes")) %>%                                
  filter(format(time, "%H:%M") >= "07:00" & format(time, "%H:%M") <= "20:45")

cf_dfw
```

## Environ
```{r}
environ_dfw <- environ %>%
  filter(format(time, "%H:%M") >= "07:00" & format(time, "%H:%M") <= "20:30")

environ_dfw
```


## Match time
Before we merge the file, check if both time frames are identical
```{r}
# Get unique time values from both data sets
environ_time <- unique(environ_dfw$time)
cf_time <- unique(cf_dfw$time)

# Check if all values are in each other
all_in_cf <- all(environ_time %in% cf_time) # Are all times in environ in cf?
all_in_environ <- all(cf_time %in% environ_time) # Are all times in cf in environ?

# Print results
if (all_in_cf && all_in_environ) {
  print("The time_15min columns in both datasets are identical.")
} else {
  print("The time_15min columns have differences.")
}

```
The two time columns are identical! Now I can merge them


## Merge
```{r}
merged <- cf_dfw %>%
  left_join(environ_dfw, by = "time")%>%  
  relocate(time, .before = qy_1) %>%  
  drop_na() # remove the first row having NA

merged
```

## Add DLI variable
```{r}
merged <- merged %>%
  mutate(date = as_date(time)) %>%
  group_by(date) %>%
  mutate(
    edli_1 = cumsum(eppfd_1) * 900 / 1e6,
    edli_2 = cumsum(eppfd_2) * 900 / 1e6,
    edli_3 = cumsum(eppfd_3) * 900 / 1e6,
    edli_4 = cumsum(eppfd_4) * 900 / 1e6,
    dli_1  = cumsum(ppfd_1) * 900 / 1e6,
    dli_2  = cumsum(ppfd_2) * 900 / 1e6,
    dli_3  = cumsum(ppfd_3) * 900 / 1e6,
    dli_4  = cumsum(ppfd_4) * 900 / 1e6
  ) %>%
  ungroup()

merged
write_csv(merged, "data/0422/merged.csv") 
```

## Long format
```{r}
merged_long <- merged %>%
  pivot_longer(
    cols = -c(time, temp, vpd, co2, eppfd_g, eppfdi_g, date),
    names_to = c(".value", "loc"),
    names_pattern = "(.*)_(\\d)") %>%
  mutate(loc = as.integer(loc))

merged_long
```

# EDA
Check how the environmental variables are related.

## Correlation plot
```{r}
# Estimating significance matrix
p.mat <- merged_long %>%
  dplyr::select(-time, -qy, -loc, -date) %>%  # leave explanatory variables only
  cor_pmat()  # compute p-values for the correlations

# Correlation plot
merged_long %>%
  dplyr::select(-time, -qy, -loc, -date) %>%
  cor() %>%
  ggcorrplot(hc.order = TRUE, 
             digits = 1,
             type = "lower", 
             p.mat = p.mat, 
             sig.level = 0.05,
             insig = "blank",
             lab = TRUE)

# save the image file
ggsave("output/0422/corrmat.png",
       height = 6,
       width = 6,
       bg = "white")
```

## Bivariate relationship
Check how the parameters are related to the response variable
```{r}
merged_long %>%
  dplyr::select(-time, -loc, -date) %>%
  pivot_longer(cols=-qy) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(qy ~ value,
                      data = .x) %>%
                    glance(.) %>%
                    pull(r.squared)
                  )) %>%
  arrange(desc(r2))
```

Let's check it with R2 plots
```{r}
merged_long %>%
  dplyr::select(-time, -loc, -date) %>%
  pivot_longer(cols=-qy) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(qy ~ value,
                      data = .x) %>%
                    glance(.) %>%
                    pull(r.squared)
                  )) %>%
  arrange(desc(r2)) %>%
  ungroup() %>%
  unnest(data) %>%
  ggplot(aes(x = value, 
             y = qy))+
  geom_point(shape = 21, 
             alpha = .7, 
             fill = "purple")+
  geom_smooth(method = "lm", 
              se = F, 
              color = "black", 
              size = 1)+
  facet_wrap(~name, 
             scales = "free_x", 
             ncol=2) 
```

# a. Data split
Four models will share the same splited data
 - 70% training
 - 30% testing

```{r}
set.seed(931735)

merged_split <- initial_split(merged_long, prop = .7)
merged_split
```

```{r}
merged_train <- training(merged_split)
merged_train
```

```{r}
merged_test <- testing(merged_split)
merged_test
```
Put "test set" aside and continue with the "train set" for taining.


# b. Data processing  
```{r}
enet_recipe <-
  # Defining predicted and predictor variables
  recipe(qy ~  .,
         data = merged_train) %>%
  # Removing year and site  
  step_rm(time, date, loc) %>%    
  
  # Normalizing all numeric variables except predicted variable
  step_normalize(all_numeric(), -all_outcomes())
  
enet_recipe
```

```{r weather_prep}
enet_prep <- enet_recipe %>%
  prep()

enet_prep
```

# c. Training  
## 1. Model specification 
```{r}
enet_spec <-
  # Specifying linear regression as our model type, asking to tune the hyperparameters
linear_reg(penalty = tune(),
           mixture = tune()) %>%
  # Specify the engine
set_engine("glmnet")
  
enet_spec
```

## 2. Hyper-parameter tuning  
```{r}
enet_grid <- crossing(penalty = seq(0, 
                                    100, 
                                    by = 10),
                      mixture = seq(0, 
                                    1, 
                                    by = 0.2))

enet_grid <- grid_regular(penalty(),
                          mixture(),
                          levels = list(penalty = 50,
                                        mixture = 10))
enet_grid
```

```{r}
ggplot(data = enet_grid,
       aes(x = mixture,
           y = penalty)) +
  
  geom_point()
```

Let's define our resampling strategy below, using a 5-fold cross validation approach:  
```{r}
resampling_foldcv <- vfold_cv(merged_train, v = 5)
resampling_foldcv
```

### Perform Grid search
```{r}
enet_grid_result <- tune_grid(enet_spec,
                              preprocessor = enet_recipe,
                              grid = enet_grid,
                              resamples = resampling_foldcv)
enet_grid_result
enet_grid_result$.metrics[[1]]
```


### RMSE (lower is better):
```{r RMSE}
enet_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "RMSE")
```
### RMSE (higher is better):
```{r R2}
enet_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = penalty, 
             y = mean, 
             color = factor(mixture), 
             group = factor(mixture))) +
  geom_line() +
  geom_point() + 
  labs(y = "R2")
```

Let's extract the hyperparameters from the best model as judged by 2 performance metrics:  
```{r}
# Based on lowest RMSE
best_rmse <- enet_grid_result %>%
  select_best(metric = "rmse")

best_rmse

# Based on greatest R2
best_r2 <- enet_grid_result %>%
  select_best(metric = "rsq")

best_r2
```

## 2-1. Bayesian tuning  
```{r}
param_set <- parameters(enet_spec)

param_set <- parameters(
  penalty(range = c(-4, 0)),    # log scale: 0.0001 to 1
  mixture(range = c(0.2, 1))    # focus on Lasso or Elastic Net
)


enet_bayes_result <- tune_bayes(
  object = enet_spec,
  preprocessor = enet_recipe,
  resamples = resampling_foldcv,
  param_info = param_set,
  initial = 10,  
  iter = 40,     
  metrics = metric_set(rmse, rsq),
  control = control_bayes(verbose = TRUE)
)

best_rmse_bayes <- select_best(enet_bayes_result, metric = "rmse")
best_rmse_bayes
best_r2_bayes <- select_best(enet_bayes_result, metric = "rsq")
best_r2_bayes

autoplot(enet_bayes_result) 
```


# d. Validation  
```{r}
final_spec <- linear_reg(penalty = best_r2$penelty,
                         mixture = best_r2$mixture)
final_spec
```

```{r}
final_fit <- last_fit(final_spec,
                      enet_recipe,
                      split = merged_split)

final_fit %>%
  collect_predictions()
```

### Metrics on the **test set**:
```{r}
final_fit %>%
  collect_metrics()
```

### Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = qy,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) 
```
### Coefficients:  
```{r}
final_spec %>%
  fit(qy ~ .,
         data = bake(enet_prep, merged_long)) %>%
  tidy() %>%
  arrange(desc(estimate))

```

### Variable importance:  
```{r}
final_spec %>%
  fit(qy ~ .,
         data = bake(enet_prep, merged_long)) %>%
    vi() %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  slice_head(n = 20) %>%
  ggplot(aes(x = Importance, 
             y = Variable, 
             fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

### Multicollinearity check
```{r}
lm_mod <- lm(qy ~ eppfd_g + eppfdi_g + eppfd + eppfdi + ppfd + ppfdi + edli + dli + temp + vpd + co2, data = merged_long)
vif(lm_mod)

lm_comp <- tibble(
  model = "lm_mod",
  r2 = summary(lm_mod)$r.squared)
lm_comp

lm_mod2 <- lm(qy ~ eppfd + edli + temp + vpd + co2, data = merged_long)
vif(lm_mod2)

lm_comp2 <- tibble(
  model = "lm_mod2",
  r2 = summary(lm_mod2)$r.squared)
lm_comp2
```






